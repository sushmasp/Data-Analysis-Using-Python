{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question-2:\n",
    "    \n",
    "Analysis-3: \n",
    "\n",
    "To know which SECTION of articles written in 2016 is receiving more COMMENTS in 2016 from which REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Community API Data preparation for Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in the start_dir and its root directories: 366\n"
     ]
    }
   ],
   "source": [
    "# Preparing Community API  required for Analysis from the API files downloaded:\n",
    "\n",
    "#To fetch all .json files \n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import glob\n",
    "\n",
    "cwd_path = os.getcwd() \n",
    "root_dir_path=os.path.abspath(os.path.join(cwd_path, os.pardir)) #To get the parent directory of cwd\n",
    "root=root_dir_path.replace(\"\\\\\", \"/\")\n",
    "start_dir=root+'/data/NYT/Community_API' #Parent directory\n",
    "\n",
    "all_files = []\n",
    "pattern   = \"*.json\"\n",
    "\n",
    "#To fetch all .json files present in parent directory and its child directories\n",
    "for dir,subdir,file in os.walk(start_dir):\n",
    "    all_files.extend(glob.glob(os.path.join(dir,pattern))) \n",
    "\n",
    "#print(all_files[:2])\n",
    "print('Total number of files in the start_dir and its root directories:',len(all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global row #Global variable for storing row_index of excel\n",
    "row=1 #Starting index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to fetch all the comments of a file and make an entry of required data in excel; \n",
    "\n",
    "def read_file_comment(filename):\n",
    "    import json\n",
    "    json_file=open(filename)\n",
    "    #json_file=open('F:/INFO7374-Python/Mid-term_Project/midterm/data/NYT/Archive_API\\\\2016\\\\April\\\\Archive_2016_April.json')\n",
    "    data=json.load(json_file)\n",
    "    \n",
    "    \n",
    "    # Setting rows and columns indexes\n",
    "    global row #Telling function that 'row' is a global variable\n",
    "    col = 0\n",
    "    \n",
    "    number_of_articles=len(data[0]['results']['comments'])\n",
    "    #print('Total number of articles:',number_of_articles)\n",
    "\n",
    "    for j in range(number_of_articles):\n",
    "        url=data[0]['results']['comments'][j]['assetURL'] #Url of the article for which comment was made\n",
    "        commenter=data[0]['results']['comments'][j]['userDisplayName'] \n",
    "        commenter_Location=data[0]['results']['comments'][j]['userLocation']\n",
    "        \n",
    "        worksheet.write(row, col, url)\n",
    "        worksheet.write(row, col+1, commenter)\n",
    "        worksheet.write(row, col+2, commenter_Location)\n",
    "                \n",
    "        row+=1\n",
    "    \n",
    "        j+=1\n",
    "        \n",
    "    \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file already exists.\n"
     ]
    }
   ],
   "source": [
    "#Writing the data required for analysis to excel sheet\n",
    "\n",
    "import xlsxwriter\n",
    "import os\n",
    "\n",
    "# Create an excel workbook if not present and add a worksheet.\n",
    "if not os.path.exists('Community_API_Data_For_Analysis.xlsx'):\n",
    "    workbook = xlsxwriter.Workbook('Community_API_Data_For_Analysis.xlsx')\n",
    "    \n",
    "    #Adding worksheet to the workbook\n",
    "    worksheet = workbook.add_worksheet()\n",
    "\n",
    "    #Adding headers(title) to the columns\n",
    "    worksheet.write(0, 0, 'web url of the article') #web_url\n",
    "    worksheet.write(0, 1, 'Commenter') #commenter\n",
    "    worksheet.write(0, 2, 'Commenter Location') #commenter location\n",
    "\n",
    "    #For each file, fetch the articles and make an entry of data required\n",
    "    for filename in all_files:  \n",
    "        read_file_comment(filename)\n",
    "    \n",
    "    workbook.close()\n",
    "    print('File has been written.')\n",
    "    \n",
    "else:\n",
    "    print('The file already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis-3: To know which section of articles is receiving more comments from which region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of archive API rows: 61928\n",
      "Total number of community API rows: 9410\n",
      "Length of dictionary:6\n"
     ]
    }
   ],
   "source": [
    "#Analysis-3: To know which section of articles is receiving more comments from which region\n",
    "\n",
    "#Sectionwise-Loctaionwise Comments\n",
    "import xlrd\n",
    "import datetime\n",
    "\n",
    "current_row = 0\n",
    "sheet_num = 0\n",
    "\n",
    "section_location_dictionary={} #Dictionary to store section-location-count of comments\n",
    "\n",
    "# path to the file you want to extract data from\n",
    "src1 = r'Archive_API_Data_For_Analysis.xlsx' #Archive API file prepared by me for analysis\n",
    "src2 = r'Community_API_Data_For_Analysis.xlsx' ##Community API file prepared by me for analysis\n",
    "\n",
    "book1 = xlrd.open_workbook(src1)\n",
    "book2 = xlrd.open_workbook(src2)\n",
    "\n",
    "# select the sheet that the data resides in\n",
    "arc_sheet = book1.sheet_by_index(sheet_num)\n",
    "comm_sheet = book2.sheet_by_index(sheet_num)\n",
    "\n",
    "# get the total number of rows\n",
    "arc_rows = arc_sheet.nrows\n",
    "print('Total number of archive API rows:',arc_rows-1) # num_rows-1 --> Excluding the header;\n",
    "\n",
    "comm_rows = comm_sheet.nrows\n",
    "print('Total number of community API rows:',comm_rows-1) # num_rows-1 --> Excluding the header;\n",
    "\n",
    "url_dict={}\n",
    "sec_cnt=0\n",
    "\n",
    "for j in range(arc_rows):\n",
    "    if j==0: #to skip the header\n",
    "         continue\n",
    "    if not j in url_dict:\n",
    "        url_dict[arc_sheet.cell_value(j, 0)] = j #Article url,row number in excel sheet\n",
    "\n",
    "for k in range(comm_rows):\n",
    "    if k==0: #to skip the header\n",
    "        continue\n",
    "        \n",
    "    asset_url=comm_sheet.cell_value(k, 0) #Asset url\n",
    "    location=comm_sheet.cell_value(k, 2) #Commenter location\n",
    "        \n",
    "    \n",
    "    if asset_url in url_dict.keys():  # If url of article in comment is present in 2016 url articles \n",
    "        row_number=url_dict[asset_url] #Fetch the section of the article\n",
    "        section=arc_sheet.cell_value(row_number, 2) #Section name\n",
    "        present=0\n",
    "    \n",
    "        if section in section_location_dictionary: #If section is present in dictionary\n",
    "            length=len(section_location_dictionary[section]) #Fetch the length of the list\n",
    "            if length==0: #If list is empty append a dictionary\n",
    "                section_location_dictionary[section].append({location:1})\n",
    "            else: #If list is not empty\n",
    "                for i in range(length):\n",
    "                    if location in section_location_dictionary[section][i]: #Check for a location name in all entries of the list\n",
    "                        section_location_dictionary[section][i][location]+=1 #If present, increment the number\n",
    "                        present=1\n",
    "                if present==0:\n",
    "                    section_location_dictionary[section].append({location:1}) #If not present, append a dictionary\n",
    "        else:\n",
    "            section_location_dictionary[section]=[] #If month is not present in dictionary, add it to dictionary\n",
    "            length=len(section_location_dictionary[section])\n",
    "            if length==0:\n",
    "                section_location_dictionary[section].append({location:1})\n",
    "    k+=1\n",
    "\n",
    "print('Length of dictionary i.e. Different Sections of articles:'+str(len(section_location_dictionary)))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Data is written to file.\n"
     ]
    }
   ],
   "source": [
    "#Writing Section-wise Location-wise comments written in 2016 for articles of 2016\n",
    "\n",
    "#Writing the output of the analysis to excel\n",
    "\n",
    "import xlsxwriter\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd() #To access the path of current working directory\n",
    "path=cwd+'/ana_3'\n",
    "\n",
    "# Create an excel workbook if not present and add a worksheet.\n",
    "if not os.path.exists(path+'/'+'Analysis_3_Sectionwise_Loctaionwise_Comments.xlsx'):\n",
    "    workbook = xlsxwriter.Workbook(path+'/'+'Analysis_3_Sectionwise_Loctaionwise_Comments.xlsx')\n",
    "\n",
    "    #Adding worksheet to the workbook\n",
    "    worksheet = workbook.add_worksheet('Section_Loctaionwise_Comments')\n",
    "\n",
    "    #Adding headers(title) to the columns\n",
    "    worksheet.write(0, 0, 'Section')\n",
    "    worksheet.write(0, 1, 'Commenter Location')\n",
    "    worksheet.write(0, 2, 'Total Number of Articles')\n",
    "\n",
    "    # Setting rows and columns indexes\n",
    "    row = 1\n",
    "    col = 0\n",
    "\n",
    "    dict_length=len(section_location_dictionary)\n",
    "\n",
    "    for key in section_location_dictionary.keys():\n",
    "        worksheet.write(row, col, key)#Section name\n",
    "    \n",
    "        location_length=len(section_location_dictionary[key])\n",
    "        for i in range(location_length):\n",
    "            for loc_key,value in section_location_dictionary[key][i].items():\n",
    "                worksheet.write(row, col+1, loc_key) #Location name\n",
    "                worksheet.write(row, col+2, value) #Count of articles\n",
    "\n",
    "            row+=1\n",
    "    workbook.close()\n",
    "    print('Output Data is written to file.')\n",
    "    \n",
    "else:\n",
    "    print('The file already exists.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Data is written to file.\n"
     ]
    }
   ],
   "source": [
    "#Writing the most and least comenting locations section-wise\n",
    "\n",
    "import xlsxwriter\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd() #To access the path of current working directory\n",
    "path=cwd+'/ana_3'\n",
    "\n",
    "# Create an excel workbook if not present and add a worksheet.\n",
    "if not os.path.exists(path+'/'+'Analysis_3_Ouput_Sectionwise_Most_Least_Commenting_Locations.xlsx'):\n",
    "    workbook = xlsxwriter.Workbook(path+'/'+'Analysis_3_Ouput_Sectionwise_Most_Least_Commenting_Locations.xlsx')\n",
    "    \n",
    "    #Adding worksheet to the workbook\n",
    "    worksheet = workbook.add_worksheet('Most_Least_Comments_Sectionwise')\n",
    "    \n",
    "    #Adding headers(title) to the columns\n",
    "    worksheet.write(0, 0, 'Section')\n",
    "    worksheet.write(0, 1, 'Location with Most number of comments')\n",
    "    worksheet.write(0, 2, 'Location with Least number of comments')\n",
    "\n",
    "    # Setting rows and columns indexes\n",
    "    row = 1\n",
    "    col = 0\n",
    "\n",
    "    for key in section_location_dictionary.keys():\n",
    "        new_dict={} #Dictionary to store the values of each section\n",
    "        cnt_list=[]\n",
    "        location_length=len(section_location_dictionary[key])\n",
    "        worksheet.write(row,col,key) #Section name\n",
    "        for i in range(location_length):\n",
    "            for loc_key,value in section_location_dictionary[key][i].items():\n",
    "                if loc_key not in new_dict.keys():\n",
    "                    new_dict[loc_key]=value\n",
    "                    cnt_list.append(value) #List fetching only the comment counts\n",
    "                    \n",
    "        if all(cnt_list[0] == item for item in cnt_list)==False: #If list  contains different entries i.e. \n",
    "                                                            #if there are locations with unique comment count\n",
    "                    \n",
    "            #Locations with most comments\n",
    "            #Descending order\n",
    "            from operator import itemgetter\n",
    "            desc_sort_dict=sorted(new_dict.items(), key=itemgetter(1),reverse=True) \n",
    "            worksheet.write(row,col+1,desc_sort_dict[0][0]+'('+str(desc_sort_dict[0][1])+')') #Top location name(number of comments)\n",
    "            \n",
    "            #Locations with least comments\n",
    "            #Ascending order\n",
    "            from operator import itemgetter\n",
    "            asc_sort_dict=sorted(new_dict.items(), key=itemgetter(1),reverse=False) \n",
    "            worksheet.write(row,col+2,asc_sort_dict[0][0]+'('+str(asc_sort_dict[0][1])+')') #Bottom location name(number of comments)\n",
    "    \n",
    "        else: #If there are locations with same comment count, print both locations as his locations_with_most_comments\n",
    "            value_to_be_entered=\"\" #Variable that stores all locations with same article count\n",
    "            #Descending order\n",
    "            from operator import itemgetter\n",
    "            desc_sort_dict=sorted(new_dict.items(), key=itemgetter(1),reverse=True) \n",
    "            for i in range(len(desc_sort_dict)):\n",
    "                value_to_be_entered+=desc_sort_dict[i][0]+'('+str(desc_sort_dict[i][1])+')'\n",
    "                if i<len(desc_sort_dict)-1:\n",
    "                    value_to_be_entered+=' , '\n",
    "\n",
    "            worksheet.write(row,col+1,value_to_be_entered) #Top location name(number of comments)\n",
    "    \n",
    "            \n",
    "        row+=1\n",
    "    \n",
    "    workbook.close()\n",
    "    print('Output Data is written to file.')\n",
    "    \n",
    "else:\n",
    "    print('The file already exists.')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
